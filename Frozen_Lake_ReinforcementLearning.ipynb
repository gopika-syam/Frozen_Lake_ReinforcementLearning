{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc2245a",
   "metadata": {},
   "source": [
    "## Frozen Lake Agent Implementation with Reinforcement Learning"
   ]
  },
  {
   "attachments": {
    "download.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c08d41a1",
   "metadata": {},
   "source": [
    "##### This program tries to implement an agent that plays Frozen Lake by walking only on the frozen tiles and avoiding holes to reach goal state with Deep Reinforcement Learning by implementing Q table\n",
    "\n",
    "The surface is described as:-\n",
    "![image.png](attachment:download.png)\n",
    "\n",
    "\n",
    "This grid represents the environment, with S as the agent's starting point which is safe. F stands for the frozen surface and is also a safe . H stands for a hole, and stepping into a hole is not safe. Finally, G stands for the goal, which is the grid spot where the prized frisbee resides.\n",
    "The agent can go left, right, up, and down, and the episode concludes when the agent achieves the goal or falls into a hole. If it achieves the goal, it earns a one-point reward; otherwise, it receives a zero-point reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc54cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e51b6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#Setting up the environement\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "print(env)\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(action_size)\n",
    "print(state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "675d90a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Creating Q table and initialising all the Q-values to zero for each state-action pair.\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec485eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paramater initialisation\n",
    "total_episodes = 20000        \n",
    "learning_rate = 0.8           \n",
    "max_steps = 99                \n",
    "gamma = 0.95                  \n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 \n",
    "max_epsilon = 1.0             \n",
    "min_epsilon = 0.01            \n",
    "decay_rate = 0.005             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bc166e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.48485\n",
      "[[2.72410733e-01 8.46745727e-02 2.71207412e-02 1.01200007e-01]\n",
      " [7.57225144e-03 3.16070816e-03 2.61914488e-03 1.19577166e-01]\n",
      " [6.37633256e-03 4.72566726e-03 4.02838004e-03 2.60025913e-02]\n",
      " [1.67308341e-03 5.16211965e-03 6.22734402e-04 1.43855119e-02]\n",
      " [3.47245102e-01 8.26073936e-02 2.96179850e-02 5.89047706e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.20102059e-03 8.45402573e-04 4.92025269e-05 1.84368981e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.19323266e-02 8.57066031e-03 2.52409379e-02 4.34404473e-01]\n",
      " [7.10553327e-03 7.63143971e-01 2.08078809e-04 3.23942006e-01]\n",
      " [8.84844649e-01 2.66556365e-04 1.86419107e-03 4.86174880e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.85827667e-02 1.09542013e-01 5.76322804e-01 1.26134760e-03]\n",
      " [2.51544637e-01 9.80819065e-01 2.05031402e-01 2.02525418e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#To hold all the rewards\n",
    "rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    for step in range(max_steps):\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        total_rewards += reward\n",
    "        state = new_state\n",
    "        if done == True: \n",
    "            break\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "    rewards.append(total_rewards)\n",
    "print (\"Score: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37893eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "EPISODE  0\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "No of steps taken 43\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "EPISODE  1\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "No of steps taken 35\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "No of steps taken 6\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "No of steps taken 27\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "No of steps taken 29\n"
     ]
    }
   ],
   "source": [
    "#Agent playing frozen Lake\n",
    "env.reset()\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "    print(\"EPISODE \", episode)\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"No of steps taken\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
